---
title: "Treasure hunt - Doc"
format:
  html:
    code-fold: true
    fig-format: png
    toc: true
    toc-location: left
    toc-depth: 4
    toc-expand: true
jupyter: python3
---

```{python}
#| echo: false
#| label: imports
exp_label = "exp_msc"
version = "test_hr_1_test_hr_1"

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.path import Path
sys.path.append(os.path.join(os.path.dirname(os.getcwd()), "code"))
sys.path.append(os.path.join(os.path.dirname(os.getcwd()), "code", "utilities"))
from utilities.very_plotter_new import PlotCustomParams
from plot_agent_perf_giv_tau import plot_agent_perf
from plot_param_recovery import plot_param_recov_results
from plot_model_recovery import plot_model_recov_results
from plot_model_estimation import plot_model_est_results

import warnings
warnings.filterwarnings('ignore')

plot_params = PlotCustomParams()

```



## Experimental data and simulation with deterministic agents

![Participant and agent task performance](figure_1.png)


## Agent-based behavioral modelling

### Model formulation

% ------Task Model-------------
\paragraph{Task Model.}\label{sec:task_model}
The model of the task is formulated based on theory of partially observable Markov decision processes \parencite{bertsekasDynamicProgrammingOptimal2005}. The two-dimensional gridworld is represented by a graph $(I,E)$, where $I:=\mathbb{N}_{25}$ is the set of nodes corresponding to the 25 cells of the grid and $E:=\mathbb{N}_{40}$ is the set of edges corresponding to the direct connections between two adjacent nodes (see Fig. \ref{fig:graph_and_agent_task_interaction} for a visualization of the graph model). One experimental run is represented by the tuple
\begin{equation}\label{MDP}
    \mathcal{T} := (T,C,S,A,R,O,p^{s, a_{t}}(r_t),f,g)
\end{equation}
Here,
\begin{itemize}
    \item $T$ denotes the maximum number of trials per round, indexed by \textcolor{red}{$t=0,...,T$}. In accordance with task rules, a round ends when the participant finds the treasure, the number of performed trials varies dependent on whether and when the agent finds the treasure. The last trial on which an observation is returned is denoted by $t^*$.

    \item $C$ denotes the number of rounds per run, indexed by $c=1, ..., C$. One run consists of 10 rounds ($C=10$).

    \item $S:=\{s^o, s^u\}$ denotes the set of observable ($s^o:=\{s^1, s^2\}$) and unobservable ($s^u:=\{s^3, s^4\}$) states.  Note that $s^1$ and $s^2$ are round- and trial-specific, $s^3$ is only round-specific, while $s^4$ is constant over all trials and rounds of one run. Hence, $s^1_t$ and $s^2_t$ will be denoted with the subscript $t$ indexing the trial, $s^3_c$ with the subscript $c$ indexing the round, while $s^4$ will be denoted without subscript. For ease of presentation, the subscript indexing the round will be omitted for $s^1$ and $s^2$. If not explicitly defined otherwise, all definitions and equations containing $s^1_t$ or $s^2_t$ apply to all rounds $c$ of one game. If $s^1$ or $s^2$ are denoted with two subscripts (e.g. $s^1_{1,3}$), the first position refers to the round $c$ and the second to the trial $t$. The following gives a more detailed description of each state component:
        \begin{itemize}
            \item $s^1:=I$ captures the the agent's current position in the gridworld and can take on a value between 1 and 25, corresponding to the node numbers $i=1, ..., 25$. For example, $s^1_t=1$ denotes that the agent's position at trial $t$ is in the left top node, while for the lower right node, $s^1_t$ takes on the value $25$.
            \item $(s^2_i)_{i=1, ..., 25}:=\{0,1,2\}^{25}$ denotes each node's background color, where each entry $s^2_i$ of the vector can take on the values $0$, $1$ or $2$. $s^2_{i,t}=0$ encodes a black background color on node $i$ at trial $t$, indicating that node $i$'s hiding spot status as defined by $s^4$ has not been unveiled yet.  $s^2_{i,t}=1$ encodes a grey background color on node $i$ at trial $t$, indicating that node $i$ is not a hiding spot. $s^2_{i,t}=2$ encodes a green background color on node $i$ at trial $t$, indicating, that node $i$ is a hiding spot.
            \item $s^3:=I$ denotes the treasure location and can take on a value between $1$ and $25$, corresponding to the node numbers $i=1, ..., 25$, excluding the starting position $s^1_1$ of the current round. $s^3_c=i$ encodes that in round $c$, the treasure is located on node $i$.
            \item $(s^4_i)_{i=1, ..., 25}:=\{0,1,2\}^{25}$ denotes the hiding spot locations, where each entry $s^4_i$ of the vector can take on the value $0$, encoding that node $i$ is not a hiding spot or $1$, encoding that node $i$ is a hiding spot. Please refer to \fullref{sec:s4_perms} for a more detailed description of all possible permutations for $s^4$.
        \end{itemize}
    
    \item $A:=\{a^i, a^{st}\}$ captures the set of actions. $a^i=0$ corresponds to the informative action to drill. $a^{st} := \{-5,+1, +5, -1\}$ correspond to uninformative actions to take a step. More specifically, $a=-5$ denotes a step upwards, $a=+1$ a step to the right, $a=+5$ a step downwards and $a=-1$ a step to the left. For ease of presentation, uninformative actions $a \in \{-5,+1, +5, -1\}$ will hereafter also be denoted as $a\neq0$. The action set available at a given trial depends on the observable current position $s^1_t$. Available actions $A_{s^1_t}\subset A$ at trial $t$ always include drilling, but exclude those steps that would cross the boarder edges of the gridworld. The state-dependent action sets for different positions in the gridworld are summarized in Table \ref{tab:state_dep_action_set}.
        
    \begin{table}[H]
    \caption[State-dependent action sets]{\textbf{State-dependent action sets}}
    \renewcommand{\arraystretch}{1.3}
    \begin{footnotesize}
    \begin{tabular}{ m{5cm} m{4.5cm} m{4cm}}
        \midrule
        Position & $s^1$ value & Available actions $A_{s^1_t}$ \\[3pt]
        \toprule
        top left node & 1 & $\{0,+1,+5\}$	\\
        top right node & 5 & $\{0,+5,-1\}$	\\
        bottom left node & 21 & $\{0,-5, +1\}$	\\
        bottom right node & 25 & $\{0,-5, -1\}$	\\
        top row, middle columns & 2,3 and 4 & $\{0,+1,+5,-1\}$	\\
        bottom row, middle columns & 22,23 and 24 & $\{0,-5,+1,-1\}$ \\
        left column, middle rows & 6,11 and 16 & $\{0,-5,+1,+5\}$ \\
        right column, middle rows & 10, 15 and 20 & $\{0,-5,+5,-1\}$ \\
        center nodes (i.e. not boarder) & 7,8,9,12,13,14,17,18 and 19 & $\{0,-5,+1,+5,-1\}$\\ [3pt]
        \midrule
    \end{tabular}
    \vspace{3mm}
    
    Available action sets (right column) for different values for $s^1$ (middle column). The left column gives a verbal descriptions of respective nodes in the gridworld. 
    \vspace{3mm}
    \label{tab:state_dep_action_set}
    \end{footnotesize}    
    \end{table}

    \item $R:=\{1,0\}$ denotes the set of rewards, where $r=1$ means the agent gains one point in the current round, because it has found the treasure and $r=0$ corresponds to no reward.
    
    \item $O:= \mathbb{N}_3^0$ denotes the set of observations $o$ the agent can make on its current position $s^1_t$.  Corresponding to the values of $s^2$ which encode all nodes' background colors, $o_t=0$, $o_t=1$ and $o_t=2$ encode the observations of a black, grey or green background color on node $i=s^1_{t}$, respectively. $o_t=3$ encodes the image of the treasure appearing on node $i=s^1_{t}$. 

    \item $p^{s^1_{t+1}, s^3_c, a_{t}}(r_t)$ is the deterministic state- and action-dependent reward distribution. Note that the reward $r_t$ received in trial $t$ is dependent on state $s^1_{t+1}$ which refers to the new position at the beginning of the subsequent trial $t+1$ following an action $a_t$ made in the current trial. After informative actions ($a_{t} = 0$), the reward probability is independent of states. Formally, 
    \begin{equation}\label{eq:reward_task_a0}
        p^{s^1_{t+1},s^3_c,a_{t}=0}(r_t) = p^{a_{t-1}=0} = 0
    \end{equation}\label{eq:reward_task_a1}
    After non-informative actions ($a_{t} \neq 0)$, on the the other hand, the reward probability depends on both the new position ($s^1_{t+1}$) and current round's treasure location ($s^3_c$). Formally, 
    \begin{equation}
        p^{s^1_{t+1},s^3_c, a_{t}\neq 0}(r_t) :=
        \begin{cases}
        1, & \text{ if } s^1_{t+1} = s^3_c \\
        0, & \text{ else}
        \end{cases}
    \end{equation}

    \item $f$ specifies the state-transition function. Formally, 
        \begin{equation}\label{state-transition-function_0}
            f:S \times A \rightarrow S, (s_t, a_{t}) \mapsto f(s_t, a_{t}) = s_{t+1} := (s^1_*, s^2_*, s^3_c, s^4),
        \end{equation}
        where
        \begin{equation}\label{eq:s1_transition}
            s^1_* = s^1_t + a_{t}
        \end{equation}
        represents the movement to a new node and
         \begin{equation}\label{eq:s2_transition}
            s^2_{*,i} :=
            \begin{cases}
                s^2_{i,t} + s^4_{i,t} + 1, & \text{if } a_{t}=0 \\
                s^2_{i,t}, & \text{ else }
            \end{cases}
        \end{equation}
        represents the change of the background color on node $i=s^1_{t+1}=s^1_{t}$ after drilling. After each action at trial $t$, the observable state components $s^1$ and $s^2$ in the change at trial $t+1$ according to Eq. \eqref{eq:s1_transition} and Eq. \eqref{eq:s2_transition}, respectively. Note that since the action value for drilling equals $0$, the state transition for $s^1$ in Eq. \eqref{eq:s1_transition} after drills takes the form $s^1_{t+1}=s^1_t + 0 = s^1_{t}$, representing that the agent's position in trial $t+1$ will remains the same as in trial $t$. 
        The transition of the second state component $(s^2_i)_{i=1, ..., 25}$ (Eq. \eqref{eq:s2_transition}, top) will only affect the entry that corresponds to the current position (i.e. $s^2_{i=s^1_t}$) and will only occur after informative actions ($a_{t} = 0$). The bottom row of Eq. \eqref{eq:s2_transition} describes that after uninformative actions ($a \neq 0$) the $i$'th value of $s^2_{t+1}$ in trial $t+1$ for $i=s^1_{t+1}=s^1_{t}$ will remain the same as in trial $t$, representing that the background colors does not change after step actions. The choice of an action determines the subsequent state with certainty, 
        \begin{equation}\label{state-transition-function_1}
            p\left(s_{t+1}=(s^1_*, s^2_*, s^3_c, s^4)\vert s_{t}, a_{t}\right):= 1.
        \end{equation}

    \item The observation function $g$ specifies the agent's observation on node $i = s^1_{t}$ at trial $t$. Importantly, the observation $o_t$ is returned to the agent before its action $a_t$ at trial $t$. Consequently, if the agent chooses an step action at trial $t$ and the new position $s^1_{t+1}$ contains the treasure, the observation of the treasure image is returned in the subsequent trial $t+1$ by $o_{t+1}=3$. To account for cases in which the treasure discovery occurs after the action in the last trial of a round ($t=T=12$), the trial index for observations is extended to $T+1$. In those cases $13$ observations are recorded for the respective round. This additional observation is necessary to implement an additional belief state update that accounts for the treasure discovery. Please refer to the specification of belief states updates (Eq. \eqref{eq:bs_update}) in the \textit{Bayesian agents} section below, for a more detailed explanation of how the treasure image observation in integrated in belief states and to Fig. \ref{fig:graph_and_agent_task_interaction} for a visualization of the agent-task-interaction and the order in which task and agent functions are evaluated over trials. The observation function takes the form
    \begin{equation}\label{eq:obs_function}
        g: S \rightarrow O, (s_t) \mapsto g(s_t) := o_t = 
        \begin{cases}
            s^2_{i=s^1_t, t}, & \text{ if } s^1_{t} \neq s^3_c \\
            3, & \text{ if } s^1_{t} = s^3_c, 
        \end{cases}
    \end{equation}
   for $t=1,...,t^*$. If the current position $s^1_t$ is not the treasure location, the observation  takes on values 0, 1 or 2, directly corresponding to the the new position's background color as denoted by $(s^2_i)_{i=1, ..., 25}$. If the new position is the treasure location, $o_t$ takes on the value $3$, corresponding to the treasure image appearing on the current position $s^1_t$. 
\end{itemize}



### Model and parameter recovery

#### Agent task performance

```{python}
#| label: agent-task-performance
#| fig-cap: "Agent task performance"

plot_agent_perf(exp_label="exp_msc",
                vers="50parts_new",
                save_file=False)
```

### Parameter Recovery

```{python}
#| label: fig-param-recov
#| fig-cap: "Parameter recovery"
#| output : true

plot_param_recov_results(exp_label=exp_label,
                         vers="test_hr",
                         save_file=False)
```

#### Model Recovery

```{python}
#| label: fig-model-recov
#| fig-cap: "Model recovery"
#| output : true

plot_model_recov_results(exp_label=exp_label,
                         #vers="test_parallel_1",
                         vers="test_hr",
                         save_file=False)
```


#### Model estimation and comparison
```{python}
#| label: fig-model-estimation
#| fig-cap: "Model estimation with experimental data"
#| output : true

plot_model_est_results(exp_label=exp_label,
                       vers="test_1",
                       save_file=False)
```